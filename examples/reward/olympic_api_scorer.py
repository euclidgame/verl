# Copyright 2025
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Custom reward scorer that queries a remote HTTP endpoint.

To enable this scorer in VeRL training, set in your Hydra overrides:

```
reward_model.enable=false   # skip local RewardModelWorker
custom_reward_function:
  path: examples/reward/olympic_api_scorer.py
  name: compute_score
```

Environment variables:
    OLYMPIC_SCORE_ENDPOINT – Base URL of the scoring service
    OLYMPIC_SCORE_TOKEN    – Optional bearer token for authentication
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, List, Optional
import random
from concurrent.futures import ThreadPoolExecutor

from openai import OpenAI
from tqdm.auto import tqdm  # Progress bar
import requests

client = OpenAI()

prompt_template = """
Your input will consist of the following components:
      - **Problem Statement**: A mathematical problem that the proof is attempting to solve.
      - **Proof Solution**: The proof that you need to evaluate. This proof may contain errors, omissions, or unclear steps. The proof was generated by another language model.

      ### Output format:
      Please summarize your analysis of the proof concisely. Return a json object with two fields:
      - correctness: a boolean value indicating whether the proof is correct
      - explanation: a string explaining your reasoning

      ### How the solution should be graded:
      A solution should be considered correct even if it would earn 5+/7 points in a standard grading format. Examples of small penalties
      worth 1 point are if the solution:
      - Makes a small computational mistake that can be easily fixed
      - Misses an edge case which can be easily proven/disproven
      - Skips over a step that follows without much reasoning or manual work
      Depending on the severity and the context, you may also not penalise a given error. On the other hand, a solution should be marked as
      incorrect if:
      - It marks a step as trivial, if it is not immediately obvious with little reasoning why this would be the case.
      - It omits algebra-heavy computational steps, regardless of whether or not it has outlined the methodology. Skipping shorter
      computations should be permitted.
      - Generalizes over a pattern without rigorously describing the pattern, or without proving any relevant properties.
      - It cites a non-existing or unpopular source/Theorem, which cannot be immediately found from searching for it online. Thus, any
      theorems that can be immediately found and have a Wikipedia article are allowed.
      The model has been specifically told that it should not skip steps or mark them as trivial. Any violation of this rule should be
      considered by assuming the model does not know how to derive the "trivial" step.

      ### Problem Statement:
      {problem}

      ### Model Solution:
      {solution}
"""

def get_evaluation_result(problem: str, solution: str, model_name: str = "o3") -> Optional[str]:
    prompt = prompt_template.format(problem=problem, solution=solution)
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            timeout=600,
            response_format={"type": "json_object"}
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error getting evaluation result: {e}")
        return None

def get_evaluation_result_gemini(problem: str, solution: str, model_name: str = "gemini-2.0-flash-001") -> Optional[str]:
    pass

def extract_evaluation_result(result: Optional[str]) -> float:
    if result is None:
        return 0.0
    try:
        return float(json.loads(result).get("correctness", 0))
    except Exception as e:
        print(f"Error extracting evaluation result: {e}")
        return 0.0


def compute_score(*, data_source: str, solution_str: str, ground_truth=None, extra_info: dict | None = None):
    if '</think>' in solution_str:
        solution_str = solution_str.split('</think>')[1]
    else:
        return 0.0

    result = get_evaluation_result(extra_info['question'], solution_str)
    return extract_evaluation_result(result)

def compute_score_batch(
    data_sources: List[str],
    solution_strs: List[str],
    ground_truths: List[str],
    extra_infos: List[dict],
) -> List[float]:
    """Batched wrapper around ``compute_score`` using a thread pool.

    Runs scoring in parallel and shows a progress bar.
    """

    futures = []
    results: List[float] = []

    with ThreadPoolExecutor(max_workers=128) as executor:
        for data_source, solution_str, ground_truth, extra_info in zip(
            data_sources, solution_strs, ground_truths, extra_infos, strict=True
        ):
            future = executor.submit(
                compute_score,
                data_source=data_source,
                solution_str=solution_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )
            futures.append(future)

        with tqdm(total=len(futures)) as pbar:
            for future in futures:
                results.append(future.result())
                pbar.update(1)

    return results